{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 128)               768       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 16)                1040      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,081\n",
      "Trainable params: 10,081\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "\n",
    "import keras\n",
    "from keras.models       import Sequential\n",
    "from keras.layers       import Dense\n",
    "from keras.optimizers   import Adam\n",
    "\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "disable_eager_execution()\n",
    "\n",
    "# env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "# env = gym.make(\"LunarLander-v2\", render_mode=\"rgb_array\") \n",
    "# # WSL2 에서 pygame 화면 출력이 안되던 문제는, anaconda 가상환경의 화면 출력 라이브러리 관련 문재 였음. \n",
    "# 현재는 가상환경 사용을 배제 하거나, 가상환경용 라이브러리 폴더에 필요한 라이브러리 파일을 리눅스 라이브러리에서 복사해 옴으로써 해결 \n",
    "\n",
    "\"\"\"\n",
    "문제 분석\n",
    "\n",
    "    문제 : LunarLander-v2\n",
    "\n",
    "    특성 : \n",
    "        보상이 희박하다는 특성이 있음\n",
    "        각 시나리오마다 상황이 종료되고 나서야 문제를 해결할 수 있음.\n",
    "\n",
    "        일정한 바람, 난류 등, 외란이 있음\n",
    "\n",
    "\n",
    "\n",
    "풀이 전략\n",
    "    방법\n",
    "        신경망 응용 강화학습\n",
    "\n",
    "        함수 평가\n",
    "            MC 방법으로 접근 \n",
    "            # incremental MC 방법으로 접근 \n",
    "                장점 가장 간단한 방법\n",
    "\n",
    "        네트워크 설계\n",
    "            기존에 존재하는 손실함수 그대로 사용하기 위해\n",
    "            비효율적이지만 가장 고전적인 방법으로 접근. \n",
    "            \n",
    "            네트워크 입력\n",
    "                상황, 행동\n",
    "            네트워크 출력\n",
    "                리워드\n",
    "\n",
    "    학습결과 실행\n",
    "        상황과 행동에 따라 리워드를 예측하는 함수가 학습하였으니,\n",
    "        각 상황마다. 4가지 행동을 대입하여 그중 무엇이 가장 효과적인지 비교후 최고를 선택하는 방법적용.\n",
    "\n",
    "    \n",
    "    문제\n",
    "        \n",
    "        1. 학습자체는 실행이 되나, 학습된 내용에 문제가 있음\n",
    "\n",
    "        reward = reward_sum\n",
    "        reward = reward_sum**2\n",
    "        reward = reward_sum**5\n",
    "        reward = reward**(eta*frame)\n",
    "        등등의 방법은 항상 마지막 실패 상태에서 보상이 가장 크도록 첵정되어\n",
    "        넘어지도록 유도하는 행동이 오히려 강하게 학습되는 경향이 발생한다.\n",
    "        \n",
    "\n",
    "        2. MC 방법과 Incremental MC 방법\n",
    "            두 방법 모두 시도해 보앗으나, \n",
    "            완전한 렌덤 시나리오, 모델이 제어하는 시나리오 \n",
    "            모두 학습이적절치 않음. \n",
    "            reward 자체에 문제가 크면 학습이 어려운 듯.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "            \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def drive_env_random(env_arg, num_of_frame_arg, model_arg):\n",
    "    observation, info = env_arg.reset()\n",
    "\n",
    "    # check action and observation space size\n",
    "    action_sapce_size = env_arg.env.action_space.n\n",
    "    action_sapce = np.array(range(action_sapce_size))\n",
    "    observation_sapce_size = env_arg.env.observation_space._shape[0]\n",
    "\n",
    "    # create data space \n",
    "    x_input_rtn = np.zeros((num_of_frame_arg, observation_sapce_size+1))\n",
    "    y_return = np.zeros((num_of_frame_arg, 1))\n",
    "    y_output_rtn = np.zeros((num_of_frame_arg, 1))\n",
    "\n",
    "    #define temporal parameters\n",
    "    reward_sum = 0\n",
    "    frame = 0\n",
    "    frame_set = 0\n",
    "    \n",
    "    #define hyper parameters\n",
    "    gamma = 0.95#Q value discount\n",
    "    alpha = 0.8#Q_function update ratio\n",
    "    \n",
    "    mode = 0 # 0 : SARSA, 1 : Q_learning\n",
    "\n",
    "    # pick initial action\n",
    "    action_value_all = perdict_model(model_arg, observation, action_sapce)\n",
    "    action_index = action_value_all.argmax()\n",
    "    action_pick = env_arg.action_space.sample()\n",
    "    \n",
    "    for i in range(num_of_frame_arg):\n",
    "        frame += 1\n",
    "\n",
    "        observation, reward, terminated, truncated, info = env_arg.step(action_pick)\n",
    "        \n",
    "        #pick next state and action ; which will be used in next iteration \n",
    "        action_value_all = perdict_model(model_arg, observation, action_sapce)\n",
    "        action_index = action_value_all.argmax()\n",
    "        action_pick = env_arg.action_space.sample()\n",
    "        # action_value 중 최대 값을 선택하면 Q-learnig, action_value의 평균값을 사용하면 SARSA\n",
    "\n",
    "        if mode == 0:\n",
    "            action_value = action_value_all.mean()#SARSA mode\n",
    "        else : \n",
    "            action_value = action_value_all[action_index]# Q_learning mode\n",
    "\n",
    "        # if(reward_sum < 10):#reward seturation\n",
    "        #     reward_sum += reward\n",
    "        x_input_rtn[i, 0:observation_sapce_size] = observation[:] # 파이썬 인덱싱에 주의할 것. 인덱스가 1:4 이면 끝나는 인덱스가 0번 부터 3번까지\n",
    "        x_input_rtn[i, observation_sapce_size:observation_sapce_size+1] = action_pick\n",
    "        if terminated or truncated:\n",
    "            if terminated == 1:\n",
    "                reward = -5\n",
    "\n",
    "            observation, info = env_arg.reset()\n",
    "            frame_set = i+1\n",
    "            frame = 0\n",
    "            reward_sum = 0\n",
    "        \n",
    "        y_return[i, :] = reward\n",
    "        # print(\"=================\")\n",
    "        # print(Q_target)\n",
    "        # print(y_output_rtn[i, :])\n",
    "        # y_output_rtn[i, :] = alpha*y_output_rtn[i, :] + (1-alpha)*Q_target\n",
    "        # print(y_output_rtn[i, :])\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print(\"{}, {} and {} %\".format(i, num_of_frame_arg, round((i/num_of_frame_arg)*100)))\n",
    "        \n",
    "        if terminated or truncated == 1:\n",
    "            break\n",
    "\n",
    "    for i in range(frame_set):\n",
    "        return_sum = 0\n",
    "        for j in range(i, frame_set):\n",
    "            return_sum +=  y_return[j, :]*(gamma**(j-i))\n",
    "        print(return_sum)\n",
    "        \n",
    "        # Q_value = model_arg.predict(x_input_rtn[i])\n",
    "        # y_output_rtn[i, :] = (alpha)*Q_value + (1-alpha)*return_sum\n",
    "        Q_value = model_arg.predict(x_input_rtn[i].reshape(1,observation_sapce_size+1))\n",
    "        y_output_rtn[i, :] = (alpha)*Q_value + (1-alpha)*return_sum\n",
    "\n",
    "    #rolling_queue\n",
    "\n",
    "    return x_input_rtn, y_output_rtn\n",
    "\n",
    "\n",
    "\n",
    "def drive_env_by_model(env_arg, num_of_frame_arg, model_arg):    \n",
    "    observation, info = env_arg.reset()\n",
    "\n",
    "    # check action and observation space size\n",
    "    action_sapce_size = env_arg.env.action_space.n\n",
    "    action_sapce = np.array(range(action_sapce_size))\n",
    "    observation_sapce_size = env_arg.env.observation_space._shape[0]\n",
    "\n",
    "    # create data space \n",
    "    x_input_rtn = np.zeros((num_of_frame_arg, observation_sapce_size+1))\n",
    "    y_return = np.zeros((num_of_frame_arg, 1))\n",
    "    y_output_rtn = np.zeros((num_of_frame_arg, 1))\n",
    "\n",
    "    #define temporal parameters\n",
    "    reward_sum = 0\n",
    "    frame = 0\n",
    "    frame_set = 0\n",
    "    \n",
    "    #define hyper parameters\n",
    "    gamma = 0.95#Q value discount\n",
    "    alpha = 0.8#Q_function update ratio\n",
    "    epsilon = 0.1\n",
    "    mode = 0 # 0 : SARSA, 1 : Q_learning\n",
    "\n",
    "    # pick initial action\n",
    "    # action_value_all = perdict_model(model_arg, observation, action_sapce)\n",
    "    # action_index = action_value_all.argmax()\n",
    "    action_pick = pick_action(model_arg, observation, action_sapce)\n",
    "    \n",
    "    for i in range(num_of_frame_arg):\n",
    "        \n",
    "        observation, reward, terminated, truncated, info = env_arg.step(action_pick)\n",
    "        \n",
    "        #pick next state and action ; which will be used in next iteration \n",
    "        # action_value_all = perdict_model(model_arg, observation, action_sapce)\n",
    "        # action_index = action_value_all.argmax()\n",
    "        if(np.random.rand() < epsilon):\n",
    "            action_pick = env_arg.action_space.sample()\n",
    "        else:\n",
    "            action_pick = pick_action(model_arg, observation, action_sapce)\n",
    "\n",
    "        # action_value 중 최대 값을 선택하면 Q-learnig, action_value의 평균값을 사용하면 SARSA\n",
    "\n",
    "        # if mode == 0:\n",
    "        #     action_value = action_value_all.mean()#SARSA mode\n",
    "        # else : \n",
    "        #     action_value = action_value_all[action_index]# Q_learning mode\n",
    "\n",
    "        # if(reward_sum < 10):#reward seturation\n",
    "        #     reward_sum += reward\n",
    "        x_input_rtn[i, 0:observation_sapce_size] = observation[:] # 파이썬 인덱싱에 주의할 것. 인덱스가 1:4 이면 끝나는 인덱스가 0번 부터 3번까지\n",
    "        x_input_rtn[i, observation_sapce_size:observation_sapce_size+1] = action_pick\n",
    "        if terminated or truncated:\n",
    "            if terminated == 1:\n",
    "                reward = -5\n",
    "\n",
    "            observation, info = env_arg.reset()\n",
    "            frame_set = i+1\n",
    "            frame = 0\n",
    "            reward_sum = 0\n",
    "        \n",
    "        y_return[i, :] = reward\n",
    "        # print(\"=================\")\n",
    "        # print(Q_target)\n",
    "        # print(y_output_rtn[i, :])\n",
    "        # y_output_rtn[i, :] = alpha*y_output_rtn[i, :] + (1-alpha)*Q_target\n",
    "        # print(y_output_rtn[i, :])\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print(\"{}, {} and {} %\".format(i, num_of_frame_arg, round((i/num_of_frame_arg)*100)))\n",
    "        \n",
    "        if terminated or truncated == 1:\n",
    "            break\n",
    "\n",
    "    for i in range(frame_set):\n",
    "        return_sum = 0\n",
    "        for j in range(i, frame_set):\n",
    "            return_sum +=  y_return[j, :]*(gamma**(j-i))\n",
    "        print(return_sum)\n",
    "        print(type(x_input_rtn[i]))\n",
    "        print(x_input_rtn[i].shape)\n",
    "        print(x_input_rtn[i])\n",
    "        Q_value = model_arg.predict(x_input_rtn[i].reshape(1,observation_sapce_size+1))\n",
    "        y_output_rtn[i, :] = (alpha)*Q_value + (1-alpha)*return_sum\n",
    "\n",
    "    #rolling_queue\n",
    "\n",
    "    return x_input_rtn, y_output_rtn\n",
    "\n",
    "\n",
    "\n",
    "def create_model(env_arg):\n",
    "    observation_sapce_size = env_arg.env.observation_space._shape[0]\n",
    "\n",
    "    model_rtn = Sequential()\n",
    "    model_rtn.add(keras.Input(shape=(observation_sapce_size+1)))\n",
    "    model_rtn.add(Dense(64, activation='sigmoid'))\n",
    "    model_rtn.add(Dense(64, activation='relu'))\n",
    "    model_rtn.add(Dense(16, activation='relu'))\n",
    "    model_rtn.add(Dense(1, activation='relu'))\n",
    "    model_rtn.compile(loss='mse', optimizer=Adam())\n",
    "    \n",
    "    return model_rtn\n",
    "\n",
    "def create_model_sigmoid(env_arg):\n",
    "    observation_sapce_size = env_arg.env.observation_space._shape[0]\n",
    "\n",
    "    model_rtn = Sequential()\n",
    "    model_rtn.add(keras.Input(shape=(observation_sapce_size+1)))\n",
    "    model_rtn.add(Dense(128, activation='sigmoid'))\n",
    "    model_rtn.add(Dense(64, activation='sigmoid'))\n",
    "    model_rtn.add(Dense(16, activation='sigmoid'))\n",
    "    model_rtn.add(Dense(1, activation='sigmoid'))\n",
    "    model_rtn.compile(loss='mse', optimizer=Adam())\n",
    "    \n",
    "    return model_rtn\n",
    "\n",
    "def create_model_relu(env_arg):\n",
    "    observation_sapce_size = env_arg.env.observation_space._shape[0]\n",
    "    \n",
    "    model_rtn = Sequential()\n",
    "    model_rtn.add(keras.Input(shape=(observation_sapce_size+1)))\n",
    "    model_rtn.add(Dense(128, activation='relu'))\n",
    "    model_rtn.add(Dense(64, activation='relu'))\n",
    "    model_rtn.add(Dense(16, activation='relu'))\n",
    "    model_rtn.add(Dense(1, activation='relu'))\n",
    "    model_rtn.compile(loss='mse', optimizer=Adam())\n",
    "    \n",
    "    return model_rtn\n",
    "\n",
    "def learning_model(model_arg, x_input_arg, y_output_arg):\n",
    "    model_arg.fit(x_input_arg, y_output_arg, batch_size=100, epochs = 2)\n",
    "\n",
    "\n",
    "def learning_model_batch_10(model_arg, x_input_arg, y_output_arg):\n",
    "    model_arg.fit(x_input_arg, y_output_arg, batch_size=10, epochs = 2)\n",
    "\n",
    "\n",
    "def perdict_model(model_arg, observation_arg, action_sapce_arg):\n",
    "    # it retruns action values at observation\n",
    "    # model_arg : keras model\n",
    "    # observation_arg : np.array (1, 8)\n",
    "    # action_sapce_arg : np.array (1, 4) \n",
    "    \n",
    "    action_space_size = action_sapce_arg.shape[0]\n",
    "    observation_space_size = observation_arg.shape[0]\n",
    "\n",
    "    action_value_rtn = np.empty((action_space_size))\n",
    "    model_input = np.empty((1, observation_space_size + 1))\n",
    "    \n",
    "    # print(model_input.shape)\n",
    "    for i in range(action_space_size):#action_space_arg의 열길이에 따라 평가를 반복\n",
    "        action = action_sapce_arg[i]\n",
    "        # print(\"================\")\n",
    "        # print(observation_arg)\n",
    "        # print(action)\n",
    "        model_input[0, 0:observation_space_size] = observation_arg  # 0~3번 인덱스\n",
    "        model_input[0, observation_space_size:observation_space_size+1] = action             #4번 인덱스\n",
    "        # print(model_input)\n",
    "        # print(model_input.shape)\n",
    "        \n",
    "        action_value_rtn[i] = model_arg.predict(model_input, verbose = 0) # model input은 \"1차원 길이 (9)\" 형태가 아닌 2차원 크기 (1, 9) 로 사용하여야 한다. \n",
    "\n",
    "    return action_value_rtn\n",
    "\n",
    "def pick_action(model_arg, observation_arg, action_sapce_arg):\n",
    "    action_value = perdict_model(model_arg, observation_arg, action_sapce_arg)\n",
    "    action_index = action_value.argmax()\n",
    "    action_pick = action_sapce_arg[action_index]\n",
    "\n",
    "    return action_pick\n",
    "\n",
    "def pick_action_verbose(model_arg, observation_arg, action_sapce_arg):\n",
    "    action_value = perdict_model(model_arg, observation_arg, action_sapce_arg)\n",
    "    action_index = action_value.argmax()\n",
    "    action_pick = action_sapce_arg[action_index]\n",
    "\n",
    "    print(\"=================\")\n",
    "    print(type(action_value))\n",
    "    print(action_value)\n",
    "    print(type(action_index))\n",
    "    print(action_index)\n",
    "    print(type(action_pick))\n",
    "    print(action_pick)\n",
    "\n",
    "    return action_pick\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    env_screen = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "    env =  gym.make (\"CartPole-v1\")\n",
    "\n",
    "    num_of_frame = 100\n",
    "    # model_glb = create_model(env)\n",
    "    model_glb = create_model_relu(env)\n",
    "    # model_glb = create_model_sigmoid(env)\n",
    "    keras.utils.plot_model(model_glb, \"DQN.png\")\n",
    "\n",
    "    env_screen.close()\n",
    "    env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aigym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
